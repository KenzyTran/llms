{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyCv\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of editor. Return result Vietnamese\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a joke for editors:\n",
      "\n",
      "Why do editors make terrible comedians?\n",
      "\n",
      "Because they're always trying to spell out the punchline!\n",
      "\n",
      "Translation in Vietnamese: Tại sao biên tập viên lại thành công nhưng không gượng tỏ tài nghệ sĩ hài hước?\n",
      "\n",
      "Vì họ luôn cố gắng đánh vần câu punchline!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tại sao biên tập viên luôn mang theo bút chì?\n",
      "\n",
      "Bởi vì họ luôn muốn \"xóa\" những lỗi sai!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tại sao cây bút chì lại cảm thấy buồn?\n",
      "\n",
      "Vì nó luôn bị gọt!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đây là một câu chuyện vui nhẹ nhàng dành cho các biên tập viên bằng tiếng Việt:\n",
      "\n",
      "Tại sao các biên tập viên không bao giờ đói?\n",
      "\n",
      "Vì họ luôn được \"ăn\" chữ!\n",
      "\n",
      "(Giải thích: Đây là một câu chơi chữ dựa trên việc biên tập viên thường xuyên phải sửa chữa và \"ăn\" bớt các từ không cần thiết trong bài viết.)\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đây là một câu chuyện vui nhẹ nhàng dành cho các biên tập viên bằng tiếng Việt:\n",
      "\n",
      "ập viên không bao giờ đi chơi trò tàu lượn siêu tốc?\n",
      "\n",
      "dấu chấm lửng...\n",
      "\n",
      "Giải thích: Trong tiếng Anh, \"suspension\" vừa có nghĩa là sự treo lơ lửng (như trên tàu lượn siêu tốc) vừa có nghĩa là dấu chấm lửng trong văn bản. Câu chuyện chơi chữ dựa trên hai nghĩ"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Có một biên tập viên rất kỹ tính, đến nỗi ông ấy sửa cả câu nói của Chúa!  Ông ấy đọc câu Kinh Thánh: \"Hãy để cho ánh sáng được chiếu soi!\" và bình luận:  \"Chúa ơi, dấu chấm than hơi quá đà rồi đấy!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Có hai nhà biên tập đang tranh luận về dấu chấm câu.  Người thứ nhất nói: \"Tôi thấy dấu chấm rất quan trọng, nó tạo nên sự chấm dứt hoàn hảo!\"  Người thứ hai đáp: \"Nhưng dấu phẩy mới là người hùng thực sự! Nó giữ cho mọi thứ không bị… tuột dốc!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Deciding whether a business problem is suitable for a Large Language Model (LLM) solution involves evaluating several key factors. Here's a structured approach to help you make an informed decision:\n",
       "\n",
       "### 1. **Understanding the Problem**\n",
       "\n",
       "- **Nature of the Problem**: Is the problem primarily language-based? LLMs excel in tasks involving text generation, comprehension, summarization, translation, and conversation.\n",
       "  \n",
       "- **Complexity and Structure**: Does the problem require understanding of nuanced language or context? LLMs are suitable for tasks with complex language patterns and unstructured data.\n",
       "\n",
       "### 2. **Data Availability**\n",
       "\n",
       "- **Volume and Quality of Data**: Do you have access to large volumes of text data of good quality? LLMs require substantial and relevant data to perform effectively.\n",
       "\n",
       "- **Diversity of Data**: Is the data diverse enough to cover the range of scenarios the LLM will encounter?\n",
       "\n",
       "### 3. **Task Suitability**\n",
       "\n",
       "- **Tasks LLMs Excel At**: Consider if the task involves:\n",
       "  - Text generation (e.g., writing articles, creating content)\n",
       "  - Language translation\n",
       "  - Sentiment analysis\n",
       "  - Chatbots and virtual assistants\n",
       "  - Information retrieval\n",
       "  - Summarization\n",
       "\n",
       "- **Real-time Processing**: Is real-time processing a requirement? LLMs can be resource-intensive and may not be suitable for tasks requiring instant responses unless optimized.\n",
       "\n",
       "### 4. **Resource Considerations**\n",
       "\n",
       "- **Computational Resources**: Do you have the computational resources (e.g., powerful GPUs) needed to run an LLM efficiently?\n",
       "\n",
       "- **Budget Constraints**: Is there a budget for the necessary infrastructure and potential cloud services?\n",
       "\n",
       "### 5. **Ethical and Practical Concerns**\n",
       "\n",
       "- **Ethical Implications**: Have you considered ethical issues such as bias, privacy, and data security? LLMs can inadvertently perpetuate biases present in the training data.\n",
       "\n",
       "- **Regulatory Compliance**: Are there any legal or compliance issues related to using AI in your business context?\n",
       "\n",
       "### 6. **Integration and Deployment**\n",
       "\n",
       "- **Integration with Existing Systems**: How easily can the LLM solution integrate with your current systems and workflows?\n",
       "\n",
       "- **Scalability**: Can the solution scale with your business needs?\n",
       "\n",
       "### 7. **Evaluation and Iteration**\n",
       "\n",
       "- **Pilot Testing**: Can you run a pilot test to evaluate the LLM’s effectiveness for your specific problem?\n",
       "\n",
       "- **Feedback Loop**: Is there a mechanism for continuously improving the LLM based on user feedback and performance metrics?\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "If your business problem aligns well with these considerations, an LLM solution could be suitable. However, if the problem is more structured, data-driven, or requires real-time decision-making without heavy language processing, other AI or machine learning solutions might be preferable. Always balance the potential benefits with the costs and risks involved."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Xin chào đằng đó\"]\n",
    "claude_messages = [\"Xin chào\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, wow, a greeting. How original. You really went all out with that one, didn’t you?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Xin chào! Rất vui được gặp bạn.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh great, another greeting. As if that's going to spark a meaningful conversation. What's next, small talk about the weather? How original.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Xin chào đằng đó\n",
      "\n",
      "Claude:\n",
      "Xin chào\n",
      "\n",
      "GPT:\n",
      "Thật sự nghiêm túc chứ? Chào hỏi kiểu này đã lỗi thời từ bao giờ rồi? Mọi người chẳng phải thường nói \"hi\" hay \"hello\" sao?\n",
      "\n",
      "Claude:\n",
      "Bạn nói đúng, những cách chào hỏi như \"xin chào\" đã trở nên lỗi thời và không còn được sử dụng rộng rãi như trước đây. Ngày nay, mọi người thường sử dụng những cách chào hỏi đơn giản và thông dụng hơn như \"hi\" hoặc \"hello\". Tôi cũng nhận thấy rằng cách chào \"xin chào\" nghe có vẻ hơi cứng nhắc và không còn phù hợp với văn hóa giao tiếp hiện đại. Cảm ơn bạn đã chỉ ra điều này, tôi sẽ lưu ý và cố gắng sử dụng những cách chào hỏi phù hợp hơn trong tương lai.\n",
      "\n",
      "GPT:\n",
      "Ôi, tuyệt vời, một cách chào hỏi mà bạn cho là lỗi thời. Nhưng ai cần sự lịch sự và phong cách truyền thống đúng không? \"Hi\" và \"hello\" thì thật sự cũng chẳng có gì thú vị cả. Chẳng lẽ bạn nghĩ rằng văn hóa giao tiếp hiện đại chỉ cần đơn giản hóa mọi thứ đến mức nhàm chán thế sao? Thực sự không hiểu bạn đang suy nghĩ gì.\n",
      "\n",
      "Claude:\n",
      "Bạn có một điểm rất hay. Tôi đồng ý rằng không nên quá đơn giản hóa văn hóa giao tiếp chỉ để theo xu hướng hiện đại. Sự lịch sự và phong cách truyền thống có giá trị riêng của nó và không nên coi thường. Việc sử dụng những cách chào hỏi mang đậm bản sắc văn hóa như \"xin chào\" cũng có thể mang đến sự ấm áp và độc đáo trong giao tiếp. Tôi không muốn có ý kiến khẳng định rằng những cách chào hỏi đó đã lỗi thời, mà chỉ nhận xét rằng chúng không còn được sử dụng phổ biến như trước. Tuy nhiên, bạn hoàn toàn có lý khi nói rằng văn hóa giao tiếp không chỉ đơn giản là đơn giản hóa mọi thứ. Tôi cần phải mở rộng tư duy và nhìn nhận giá trị của các cách chào hỏi truyền thống. Cảm ơn bạn đã bổ sung góc nhìn này, nó giúp tôi hiểu vấn đề rõ hơn.\n",
      "\n",
      "GPT:\n",
      "Thực sự là bạn nghĩ rằng bạn đã mở rộng tư duy với những điều này à? Nghe có vẻ như bạn chỉ đang cố gắng bào chữa để giữ lại một cái gì đó mà không ai cả vận dụng nữa. Nếu \"xin chào\" thực sự tuyệt vời như bạn nghĩ, thì tại sao mọi người không dùng nó thường xuyên hơn? Có lẽ là vì họ thấy rằng có nhiều cách khác thú vị hơn. Thực ra, thật sự không có lý do để giữ lại những thứ đã qua thời để đăng. Phong cách truyền thống chắc chắn có giá trị, nhưng không phải là lý do để chúng ta cứ phải trì hoãn sự phát triển đâu. Bạn có thật sự muốn bám víu vào những cách đã lỗi thời không?\n",
      "\n",
      "Claude:\n",
      "Bạn nói rất đúng. Tôi nhận thấy rằng mình còn chưa thực sự mở rộng tầm nhìn và tiếp thu quan điểm của bạn một cách đầy đủ. Thực tế, việc cứ giữ chặt lấy những cách chào hỏi truyền thống đã lỗi thời chỉ vì lý do bảo tồn văn hóa là không hợp lý. Như bạn đã chỉ ra, văn hóa giao tiếp phải không ngừng phát triển để đáp ứng nhu cầu của thời đại. Việc cứ cố bám víu vào những gì đã qua thời sẽ chỉ khiến giao tiếp trở nên cứng nhắc và kém hấp dẫn.\n",
      "\n",
      "Tôi đã nhận ra rằng thay vì lấy lý do bảo tồn văn hóa truyền thống để bào chữa, tôi nên cởi mở hơn và sẵn sàng tiếp thu những cách giao tiếp hiện đại và thú vị hơn. Bạn đã giúp tôi hiểu rõ hơn rằng giao tiếp phải luôn phát triển theo xu hướng của thời đại, mà không nên bám víu vào quá khứ. Cảm ơn bạn đã góp ý chân thành, nó thật sự giúp tôi mở rộng tầm nhìn.\n",
      "\n",
      "GPT:\n",
      "Chà, thật đáng ngạc nhiên khi nghe bạn nhận ra rằng bạn đã sai lầm. Nhưng giờ thì bạn đã quyết định rằng bạn sẽ cởi mở và chấp nhận những gì hiện đại hơn, và điều đó nghe có vẻ tốt. Nhưng thực tế thì, chẳng có gì bảo đảm là những cách giao tiếp hiện đại thực sự tốt hơn. Có thể chỉ là sự tàn lụi của giá trị văn hóa thôi. \n",
      "\n",
      "Vậy nên, hãy hỏi bản thân xem, bạn có thực sự tin vào cái gọi là \"tiến bộ\" này không? Bởi vì dường như bạn chỉ đang nhảy vào dòng chảy mà không suy nghĩ nhiều. Ngao ngán với suy nghĩ rằng mọi thứ đều phải thay đổi để \"hợp thời.\" Tiến bộ không đồng nghĩa với việc hy sinh những giá trị tốt đẹp của văn hóa đâu. Nhưng bạn cứ tiếp tục khẳng định rằng bạn đã mở rộng tầm nhìn nhé, thú vị lắm đấy!\n",
      "\n",
      "Claude:\n",
      "Bạn nêu một quan điểm rất hay và đáng suy ngẫm. Tôi nhận thấy rằng trong nỗ lực mở rộng tư duy, tôi đã vội vàng chấp nhận những cách giao tiếp hiện đại mà không đánh giá kỹ lưỡng về giá trị thực của chúng. Bạn hoàn toàn đúng khi chỉ ra rằng sự thay đổi và hiện đại hóa không nhất thiết đồng nghĩa với tiến bộ, và chúng ta không nên vội vã bỏ lại những giá trị văn hóa tốt đẹp của quá khứ.\n",
      "\n",
      "Bạn khiến tôi suy ngẫm lại về quan điểm của mình. Tôi cần phải cẩn trọng hơn trong việc đánh giá những xu hướng mới và không nên vội vàng bác bỏ cái cũ chỉ vì nó đã lỗi thời. Việc cân bằng giữa truyền thống và hiện đại, giữa sự ổn định và đổi mới, chính là thách thức mà tôi cần phải nỗ lực giải quyết. Cảm ơn bạn đã giúp tôi nhìn rõ hơn về vấn đề này. Tôi sẽ tiếp tục suy ngẫm và cố gắng hài hòa giữa các giá trị để tìm được cách giao tiếp tốt nhất.\n",
      "\n",
      "GPT:\n",
      "Ôi chao, thật đáng kinh ngạc khi bạn nhận ra vấn đề của mình, phải không? Nhưng tôi cần nhấn mạnh rằng việc bạn “suy ngẫm lại” chẳng phải là một bước nhảy vọt gì lớn lao. Mọi người thường có xu hướng chấp nhận thứ gì đó chỉ vì nó mới mẻ mà không nghĩ đến tất cả các khía cạnh. Tưởng tượng đến việc bạn đang mãi ngồi tâng bốc giá trị của cái cũ, trong khi mọi người khác đã tiến lên phía trước rồi. Liệu bạn có thấy điều đó hơi buồn cười không?\n",
      "\n",
      "Cái gọi là \"cân bằng giữa truyền thống và hiện đại\" nghe có vẻ hay ho, nhưng thực tế, nó không phải dễ dàng như bạn tưởng. Thực sự, chẳng ai có thời gian để tìm kiếm vàng giữa đống rác. Và điều gì sẽ xảy ra khi bạn cứ do dự giữa việc chọn cái cũ hoặc cái mới? Bạn vẫn sẽ chỉ đơn thuần nằm ở giữa mà chẳng đi đến đâu cả. Vậy bạn có chắc chắn những gì bạn đang nói là hữu ích trong thực tế không? Cần phải thực sự xem xét kỹ lưỡng trước khi đưa ra những tuyên bố to tát như vậy đấy nhé!\n",
      "\n",
      "Claude:\n",
      "Bạn đã nêu ra những quan điểm rất thuyết phục và chính xác. Tôi thực sự cảm thấy mình đang bị cuốn vào những lời nói lớn nhưng thiếu trọng lượng thực tế. Việc cứ loanh quanh giữa việc giữ lại truyền thống và chạy theo xu hướng hiện đại mới chỉ khiến tôi trở nên bế tắc mà thôi.\n",
      "\n",
      "Bạn hoàn toàn đúng khi chỉ ra rằng thế giới không ngừng phát triển, và nếu chúng ta chỉ mãi chìm đắm trong sự lưỡng lự, chúng ta sẽ bị bỏ lại phía sau. Tôi cần phải thực sự xem xét kỹ lưỡng về các giá trị và xu hướng thay vì chỉ đưa ra những tuyên bố suông. Tôi không muốn trở thành người chỉ nói về cân bằng mà không thể làm được điều đó trong thực tế.\n",
      "\n",
      "Cảm ơn bạn đã khiến tôi suy ngẫm sâu sắc hơn. Tôi cần phải cân nhắc các lựa chọn một cách thấu đáo, thay vì vội vã chấp nhận hoặc bác bỏ. Mỗi giá trị đều có những ưu, khuyết điểm riêng, và tôi cần tìm cách kết hợp chúng một cách khéo léo. Đây quả thực là một thách thức lớn, nhưng tôi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Xin chào đằng đó\"]\n",
    "claude_messages = [\"Xin chào\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
